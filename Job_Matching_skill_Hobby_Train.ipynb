{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aDv4Euus5rBp",
        "outputId": "7cf3bba4-1e18-4350-fcce-a51d9ff92daf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing data generation...\n",
            "Generated 100 samples\n",
            "Sample data:\n",
            "                                     job_description           job_category  \\\n",
            "0  We are looking for a qualified construction ma...   Construction Manager   \n",
            "1  We are looking for a qualified volunteer coord...  Volunteer Coordinator   \n",
            "2  We are looking for a qualified landscaper to j...             Landscaper   \n",
            "3  We are looking for a qualified singer to join ...                 Singer   \n",
            "4  We are looking for a qualified travel guide to...           Travel Guide   \n",
            "\n",
            "                                     required_skills  match_score  \n",
            "0  [Construction, Teamwork, Communication, Techno...     0.903431  \n",
            "1  [Volunteering, Community Service, Communicatio...     0.967833  \n",
            "2  [Gardening, Construction, Teamwork, Fitness, A...     0.929384  \n",
            "3  [Singing, Arts & Crafts, Communication, Public...     0.910177  \n",
            "4  [Traveling, Communication, Public Speaking, Te...     0.626282  \n",
            "Job categories: [np.str_('Construction Manager') np.str_('Volunteer Coordinator')\n",
            " np.str_('Landscaper') np.str_('Singer') np.str_('Travel Guide')\n",
            " np.str_('Childcare Provider') np.str_('Actor') np.str_('Game Developer')\n",
            " np.str_('Personal Trainer') np.str_('Chef') np.str_('UI/UX Designer')\n",
            " np.str_('Sports Coach') np.str_('Healthcare Worker')\n",
            " np.str_('Marketing Specialist') np.str_('Social Worker')\n",
            " np.str_('Writer') np.str_('Veterinarian') np.str_('Teacher')\n",
            " np.str_('Software Developer') np.str_('Event Coordinator')]\n",
            "\n",
            "Training the job matching model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"job_matching_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"job_matching_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ text_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │  \u001b[38;5;34m1,280,000\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │     \u001b[38;5;34m24,640\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m256\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling1d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m24,704\u001b[0m │ max_pooling1d[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m512\u001b[0m │ conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │     \u001b[38;5;34m49,408\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling1d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_1[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_max_pooli… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ global_max_pooli… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ skills_input        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m24,704\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m1,728\u001b[0m │ skills_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m512\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │        \u001b[38;5;34m256\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m192\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m49,408\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │        \u001b[38;5;34m512\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ category_output     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │      \u001b[38;5;34m2,580\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ match_output        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ text_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,640</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling1d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling1d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ global_max_pooli… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ skills_input        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,728</span> │ skills_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">192</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ category_output     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,580</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ match_output        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,493,269\u001b[0m (5.70 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,493,269</span> (5.70 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,491,733\u001b[0m (5.69 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,491,733</span> (5.69 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,536\u001b[0m (6.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,536</span> (6.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 26ms/step - category_output_accuracy: 0.6585 - category_output_loss: 1.2832 - loss: 0.9717 - match_output_loss: 0.2447 - match_output_mae: 0.4121 - val_category_output_accuracy: 0.0600 - val_category_output_loss: 3.4678 - val_loss: 2.4424 - val_match_output_loss: 0.0499 - val_match_output_mae: 0.2113 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - category_output_accuracy: 0.9993 - category_output_loss: 0.0360 - loss: 0.0592 - match_output_loss: 0.1131 - match_output_mae: 0.2697 - val_category_output_accuracy: 0.0625 - val_category_output_loss: 4.0032 - val_loss: 2.8083 - val_match_output_loss: 0.0202 - val_match_output_mae: 0.1308 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - category_output_accuracy: 0.9994 - category_output_loss: 0.0172 - loss: 0.0281 - match_output_loss: 0.0535 - match_output_mae: 0.1737 - val_category_output_accuracy: 0.1650 - val_category_output_loss: 3.5008 - val_loss: 2.4534 - val_match_output_loss: 0.0094 - val_match_output_mae: 0.0853 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - category_output_accuracy: 1.0000 - category_output_loss: 0.0099 - loss: 0.0151 - match_output_loss: 0.0272 - match_output_mae: 0.1203 - val_category_output_accuracy: 0.2775 - val_category_output_loss: 2.4450 - val_loss: 1.7138 - val_match_output_loss: 0.0077 - val_match_output_mae: 0.0724 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - category_output_accuracy: 1.0000 - category_output_loss: 0.0058 - loss: 0.0100 - match_output_loss: 0.0198 - match_output_mae: 0.0980 - val_category_output_accuracy: 0.6944 - val_category_output_loss: 0.9516 - val_loss: 0.6683 - val_match_output_loss: 0.0072 - val_match_output_mae: 0.0648 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - category_output_accuracy: 1.0000 - category_output_loss: 0.0054 - loss: 0.0084 - match_output_loss: 0.0155 - match_output_mae: 0.0847 - val_category_output_accuracy: 0.9931 - val_category_output_loss: 0.0636 - val_loss: 0.0467 - val_match_output_loss: 0.0073 - val_match_output_mae: 0.0601 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - category_output_accuracy: 0.9999 - category_output_loss: 0.0042 - loss: 0.0071 - match_output_loss: 0.0138 - match_output_mae: 0.0806 - val_category_output_accuracy: 1.0000 - val_category_output_loss: 0.0011 - val_loss: 0.0030 - val_match_output_loss: 0.0074 - val_match_output_mae: 0.0577 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - category_output_accuracy: 0.9998 - category_output_loss: 0.0029 - loss: 0.0057 - match_output_loss: 0.0123 - match_output_mae: 0.0743 - val_category_output_accuracy: 1.0000 - val_category_output_loss: 1.2696e-04 - val_loss: 0.0024 - val_match_output_loss: 0.0076 - val_match_output_mae: 0.0567 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - category_output_accuracy: 1.0000 - category_output_loss: 0.0028 - loss: 0.0053 - match_output_loss: 0.0113 - match_output_mae: 0.0705 - val_category_output_accuracy: 1.0000 - val_category_output_loss: 4.9582e-05 - val_loss: 0.0024 - val_match_output_loss: 0.0077 - val_match_output_mae: 0.0562 - learning_rate: 0.0010\n",
            "Epoch 10/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - category_output_accuracy: 1.0000 - category_output_loss: 0.0022 - loss: 0.0048 - match_output_loss: 0.0110 - match_output_mae: 0.0686 - val_category_output_accuracy: 1.0000 - val_category_output_loss: 2.8502e-05 - val_loss: 0.0024 - val_match_output_loss: 0.0079 - val_match_output_mae: 0.0558 - learning_rate: 0.0010\n",
            "Epoch 11/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - category_output_accuracy: 1.0000 - category_output_loss: 0.0017 - loss: 0.0045 - match_output_loss: 0.0110 - match_output_mae: 0.0682 - val_category_output_accuracy: 1.0000 - val_category_output_loss: 2.2467e-05 - val_loss: 0.0024 - val_match_output_loss: 0.0080 - val_match_output_mae: 0.0553 - learning_rate: 0.0010\n",
            "Epoch 12/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - category_output_accuracy: 1.0000 - category_output_loss: 0.0014 - loss: 0.0040 - match_output_loss: 0.0102 - match_output_mae: 0.0654 - val_category_output_accuracy: 1.0000 - val_category_output_loss: 1.8582e-05 - val_loss: 0.0024 - val_match_output_loss: 0.0080 - val_match_output_mae: 0.0552 - learning_rate: 0.0010\n",
            "Epoch 13/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - category_output_accuracy: 0.9999 - category_output_loss: 0.0016 - loss: 0.0044 - match_output_loss: 0.0108 - match_output_mae: 0.0664 - val_category_output_accuracy: 1.0000 - val_category_output_loss: 2.2956e-05 - val_loss: 0.0024 - val_match_output_loss: 0.0081 - val_match_output_mae: 0.0552 - learning_rate: 0.0010\n",
            "Epoch 14/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - category_output_accuracy: 1.0000 - category_output_loss: 0.0017 - loss: 0.0043 - match_output_loss: 0.0104 - match_output_mae: 0.0653 - val_category_output_accuracy: 1.0000 - val_category_output_loss: 1.5396e-05 - val_loss: 0.0024 - val_match_output_loss: 0.0081 - val_match_output_mae: 0.0550 - learning_rate: 5.0000e-04\n",
            "Epoch 15/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - category_output_accuracy: 1.0000 - category_output_loss: 0.0011 - loss: 0.0040 - match_output_loss: 0.0107 - match_output_mae: 0.0654 - val_category_output_accuracy: 1.0000 - val_category_output_loss: 1.3839e-05 - val_loss: 0.0024 - val_match_output_loss: 0.0081 - val_match_output_mae: 0.0551 - learning_rate: 5.0000e-04\n",
            "Epoch 16/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - category_output_accuracy: 1.0000 - category_output_loss: 0.0010 - loss: 0.0038 - match_output_loss: 0.0102 - match_output_mae: 0.0642 - val_category_output_accuracy: 1.0000 - val_category_output_loss: 1.1745e-05 - val_loss: 0.0024 - val_match_output_loss: 0.0081 - val_match_output_mae: 0.0550 - learning_rate: 5.0000e-04\n",
            "Epoch 17/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 28ms/step - category_output_accuracy: 1.0000 - category_output_loss: 9.7729e-04 - loss: 0.0038 - match_output_loss: 0.0104 - match_output_mae: 0.0645 - val_category_output_accuracy: 1.0000 - val_category_output_loss: 1.0915e-05 - val_loss: 0.0024 - val_match_output_loss: 0.0081 - val_match_output_mae: 0.0550 - learning_rate: 5.0000e-04\n",
            "Epoch 18/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - category_output_accuracy: 1.0000 - category_output_loss: 0.0012 - loss: 0.0038 - match_output_loss: 0.0099 - match_output_mae: 0.0635 - val_category_output_accuracy: 1.0000 - val_category_output_loss: 1.0428e-05 - val_loss: 0.0024 - val_match_output_loss: 0.0081 - val_match_output_mae: 0.0549 - learning_rate: 5.0000e-04\n",
            "Epoch 19/30\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - category_output_accuracy: 1.0000 - category_output_loss: 8.3278e-04 - loss: 0.0035 - match_output_loss: 0.0097 - match_output_mae: 0.0632 - val_category_output_accuracy: 1.0000 - val_category_output_loss: 9.5554e-06 - val_loss: 0.0024 - val_match_output_loss: 0.0081 - val_match_output_mae: 0.0549 - learning_rate: 2.5000e-04\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed successfully!\n",
            "\n",
            "Testing predictions:\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 331ms/step\n",
            "\n",
            "Test Case 1:\n",
            "Description: We need a UI/UX designer with 2 years experience in mobile app design.\n",
            "User Skills: ['Arts & Crafts', 'Technology', 'Communication']\n",
            "Predicted Category: UI/UX Designer\n",
            "Confidence: 0.719\n",
            "Match Score: 0.964\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\n",
            "Test Case 2:\n",
            "Description: Looking for a software developer to work on web applications.\n",
            "User Skills: ['Programming', 'Technology', 'Communication']\n",
            "Predicted Category: Software Developer\n",
            "Confidence: 0.770\n",
            "Match Score: 0.954\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\n",
            "Test Case 3:\n",
            "Description: Elementary teacher needed for our school.\n",
            "User Skills: ['Teaching', 'Childcare', 'Communication']\n",
            "Predicted Category: Teacher\n",
            "Confidence: 0.824\n",
            "Match Score: 0.984\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout, Concatenate, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Tuple\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class JobMatchingSystem:\n",
        "    def __init__(self, max_features=10000, max_len=200, embedding_dim=128):\n",
        "        self.max_features = max_features\n",
        "        self.max_len = max_len\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Initialize components\n",
        "        self.tokenizer = None\n",
        "        self.skill_encoder = MultiLabelBinarizer()\n",
        "        self.job_category_encoder = LabelEncoder()\n",
        "        self.model = None\n",
        "\n",
        "        # Predefined skill categories based on your app\n",
        "        self.skill_categories = [\n",
        "            'Animal Care', 'Arts & Crafts', 'Acting', 'Childcare', 'Communication',\n",
        "            'Community Service', 'Construction', 'Cooking', 'First Aid', 'Fitness',\n",
        "            'Fundraising', 'Gaming', 'Gardening', 'Health Care', 'Programming',\n",
        "            'Public Speaking', 'Reading', 'Singing', 'Social Media', 'Sports',\n",
        "            'Teaching', 'Teamwork', 'Technology', 'Traveling', 'Volunteering', 'Writing'\n",
        "        ]\n",
        "\n",
        "    def create_synthetic_data(self, num_samples=5000):\n",
        "        \"\"\"Generate synthetic training data based on realistic job-skill mappings\"\"\"\n",
        "\n",
        "        # Job categories and their typical skill requirements\n",
        "        job_skill_mapping = {\n",
        "            'Software Developer': ['Programming', 'Technology', 'Communication', 'Teamwork'],\n",
        "            'UI/UX Designer': ['Arts & Crafts', 'Technology', 'Communication', 'Programming'],\n",
        "            'Teacher': ['Teaching', 'Communication', 'Public Speaking', 'Childcare'],\n",
        "            'Healthcare Worker': ['Health Care', 'First Aid', 'Communication', 'Teamwork'],\n",
        "            'Marketing Specialist': ['Social Media', 'Communication', 'Writing', 'Arts & Crafts'],\n",
        "            'Chef': ['Cooking', 'Teamwork', 'Communication', 'Arts & Crafts'],\n",
        "            'Veterinarian': ['Animal Care', 'Health Care', 'Communication', 'First Aid'],\n",
        "            'Social Worker': ['Communication', 'Community Service', 'Volunteering', 'Public Speaking'],\n",
        "            'Personal Trainer': ['Fitness', 'Health Care', 'Communication', 'Teaching'],\n",
        "            'Construction Manager': ['Construction', 'Teamwork', 'Communication', 'Technology'],\n",
        "            'Writer': ['Writing', 'Communication', 'Reading', 'Arts & Crafts'],\n",
        "            'Childcare Provider': ['Childcare', 'Communication', 'First Aid', 'Teaching'],\n",
        "            'Event Coordinator': ['Communication', 'Fundraising', 'Public Speaking', 'Social Media'],\n",
        "            'Landscaper': ['Gardening', 'Construction', 'Teamwork', 'Fitness'],\n",
        "            'Travel Guide': ['Traveling', 'Communication', 'Public Speaking', 'Teaching'],\n",
        "            'Actor': ['Acting', 'Communication', 'Arts & Crafts', 'Public Speaking'],\n",
        "            'Singer': ['Singing', 'Arts & Crafts', 'Communication', 'Public Speaking'],\n",
        "            'Game Developer': ['Gaming', 'Programming', 'Technology', 'Arts & Crafts'],\n",
        "            'Sports Coach': ['Sports', 'Fitness', 'Teaching', 'Communication'],\n",
        "            'Volunteer Coordinator': ['Volunteering', 'Community Service', 'Communication', 'Fundraising']\n",
        "        }\n",
        "\n",
        "        # Generate job descriptions templates\n",
        "        job_descriptions = {\n",
        "            'Software Developer': [\n",
        "                \"We are looking for a skilled software developer to join our team. You will be responsible for developing and maintaining web applications.\",\n",
        "                \"Seeking an experienced programmer to work on mobile applications and web development projects.\",\n",
        "                \"Join our tech team as a software engineer. Experience with modern programming languages required.\"\n",
        "            ],\n",
        "            'UI/UX Designer': [\n",
        "                \"We need a creative UI/UX designer for our mobile app development. At least 2 years experience with web/mobile design required.\",\n",
        "                \"Looking for a talented designer to create user-friendly interfaces for our digital products.\",\n",
        "                \"Join our design team to create beautiful and functional user experiences.\"\n",
        "            ],\n",
        "            'Teacher': [\n",
        "                \"Elementary school teacher position available. Experience working with children required.\",\n",
        "                \"We are hiring passionate educators to join our school community.\",\n",
        "                \"Teaching position open for dedicated professionals who love working with students.\"\n",
        "            ],\n",
        "            'Healthcare Worker': [\n",
        "                \"Healthcare professional needed for our medical facility. First aid certification preferred.\",\n",
        "                \"Join our healthcare team to provide quality patient care.\",\n",
        "                \"Medical assistant position available. Healthcare experience required.\"\n",
        "            ],\n",
        "            'Marketing Specialist': [\n",
        "                \"Digital marketing specialist needed. Social media and content creation skills required.\",\n",
        "                \"Marketing professional wanted for brand promotion and social media management.\",\n",
        "                \"Join our marketing team to develop creative campaigns and manage online presence.\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        data = []\n",
        "        for _ in range(num_samples):\n",
        "            # Randomly select a job category\n",
        "            job_category = np.random.choice(list(job_skill_mapping.keys()))\n",
        "            required_skills = job_skill_mapping[job_category].copy()  # Make a copy to avoid modifying original\n",
        "\n",
        "            # Add some noise - sometimes add extra skills or remove some\n",
        "            if np.random.random() > 0.3:\n",
        "                # Add 1-2 random skills\n",
        "                available_skills = [s for s in self.skill_categories if s not in required_skills]\n",
        "                if available_skills:\n",
        "                    num_extra = min(np.random.randint(1, 3), len(available_skills))\n",
        "                    extra_skills = np.random.choice(\n",
        "                        available_skills,\n",
        "                        size=num_extra,\n",
        "                        replace=False\n",
        "                    ).tolist()\n",
        "                    required_skills.extend(extra_skills)\n",
        "\n",
        "            # Sometimes remove a skill\n",
        "            if len(required_skills) > 2 and np.random.random() > 0.7:\n",
        "                required_skills = required_skills[:-1]\n",
        "\n",
        "            # Generate job description\n",
        "            if job_category in job_descriptions:\n",
        "                description = np.random.choice(job_descriptions[job_category])\n",
        "            else:\n",
        "                description = f\"We are looking for a qualified {job_category.lower()} to join our team.\"\n",
        "\n",
        "            # Add some variation to descriptions\n",
        "            if np.random.random() > 0.5:\n",
        "                description += f\" Experience with {', '.join(required_skills[:2]).lower()} is preferred.\"\n",
        "\n",
        "            data.append({\n",
        "                'job_description': description,\n",
        "                'job_category': job_category,\n",
        "                'required_skills': required_skills,\n",
        "                'match_score': np.random.uniform(0.6, 1.0)  # Simulated match score\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def preprocess_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and preprocess text data\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove special characters and extra spaces\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    def prepare_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"Prepare data for training\"\"\"\n",
        "\n",
        "        # Preprocess job descriptions\n",
        "        df['processed_description'] = df['job_description'].apply(self.preprocess_text)\n",
        "\n",
        "        # Initialize and fit tokenizer\n",
        "        self.tokenizer = Tokenizer(num_words=self.max_features, oov_token=\"<OOV>\")\n",
        "        self.tokenizer.fit_on_texts(df['processed_description'])\n",
        "\n",
        "        # Convert texts to sequences\n",
        "        sequences = self.tokenizer.texts_to_sequences(df['processed_description'])\n",
        "        X_text = pad_sequences(sequences, maxlen=self.max_len)\n",
        "\n",
        "        # Encode skills\n",
        "        X_skills = self.skill_encoder.fit_transform(df['required_skills'])\n",
        "\n",
        "        # Encode job categories\n",
        "        y_categories = self.job_category_encoder.fit_transform(df['job_category'])\n",
        "\n",
        "        return X_text, X_skills, y_categories\n",
        "\n",
        "    def build_model(self, num_skills: int, num_categories: int):\n",
        "        \"\"\"Build the hybrid CNN-LSTM model for job matching\"\"\"\n",
        "\n",
        "        # Text input branch\n",
        "        text_input = Input(shape=(self.max_len,), name='text_input')\n",
        "\n",
        "        # Embedding layer\n",
        "        embedding = Embedding(\n",
        "            input_dim=self.max_features,\n",
        "            output_dim=self.embedding_dim,\n",
        "            input_length=self.max_len,\n",
        "            trainable=True\n",
        "        )(text_input)\n",
        "\n",
        "        # CNN branch for text\n",
        "        conv1 = Conv1D(64, 3, activation='relu', padding='same')(embedding)\n",
        "        conv1 = BatchNormalization()(conv1)\n",
        "        pool1 = MaxPooling1D(2)(conv1)\n",
        "\n",
        "        conv2 = Conv1D(128, 3, activation='relu', padding='same')(pool1)\n",
        "        conv2 = BatchNormalization()(conv2)\n",
        "        pool2 = MaxPooling1D(2)(conv2)\n",
        "\n",
        "        # LSTM branch for text\n",
        "        lstm = LSTM(64, return_sequences=True)(embedding)\n",
        "        lstm = Dropout(0.3)(lstm)\n",
        "        lstm_global = GlobalMaxPooling1D()(lstm)\n",
        "\n",
        "        # Global pooling for CNN\n",
        "        cnn_global = GlobalMaxPooling1D()(pool2)\n",
        "\n",
        "        # Combine CNN and LSTM features\n",
        "        text_features = Concatenate()([cnn_global, lstm_global])\n",
        "        text_features = Dense(128, activation='relu')(text_features)\n",
        "        text_features = BatchNormalization()(text_features)\n",
        "        text_features = Dropout(0.3)(text_features)\n",
        "\n",
        "        # Skills input branch\n",
        "        skills_input = Input(shape=(num_skills,), name='skills_input')\n",
        "        skills_features = Dense(64, activation='relu')(skills_input)\n",
        "        skills_features = BatchNormalization()(skills_features)\n",
        "        skills_features = Dropout(0.2)(skills_features)\n",
        "\n",
        "        # Combine all features\n",
        "        combined = Concatenate()([text_features, skills_features])\n",
        "        combined = Dense(256, activation='relu')(combined)\n",
        "        combined = BatchNormalization()(combined)\n",
        "        combined = Dropout(0.4)(combined)\n",
        "\n",
        "        combined = Dense(128, activation='relu')(combined)\n",
        "        combined = BatchNormalization()(combined)\n",
        "        combined = Dropout(0.3)(combined)\n",
        "\n",
        "        # Output layers\n",
        "        # Job category prediction\n",
        "        category_output = Dense(num_categories, activation='softmax', name='category_output')(combined)\n",
        "\n",
        "        # Match score prediction\n",
        "        match_output = Dense(1, activation='sigmoid', name='match_output')(combined)\n",
        "\n",
        "        # Create model\n",
        "        model = Model(\n",
        "            inputs=[text_input, skills_input],\n",
        "            outputs=[category_output, match_output],\n",
        "            name='job_matching_model'\n",
        "        )\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss={\n",
        "                'category_output': 'sparse_categorical_crossentropy',\n",
        "                'match_output': 'mse'\n",
        "            },\n",
        "            loss_weights={\n",
        "                'category_output': 0.7,\n",
        "                'match_output': 0.3\n",
        "            },\n",
        "            metrics={\n",
        "                'category_output': 'accuracy',\n",
        "                'match_output': 'mae'\n",
        "            }\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train(self, save_path='job_matching_model'):\n",
        "        \"\"\"Train the complete model\"\"\"\n",
        "        logger.info(\"Generating synthetic training data...\")\n",
        "        df = self.create_synthetic_data(num_samples=8000)\n",
        "\n",
        "        # Job skill mapping for match score calculation\n",
        "        job_skill_mapping = {\n",
        "            'Software Developer': ['Programming', 'Technology', 'Communication', 'Teamwork'],\n",
        "            'UI/UX Designer': ['Arts & Crafts', 'Technology', 'Communication', 'Programming'],\n",
        "            'Teacher': ['Teaching', 'Communication', 'Public Speaking', 'Childcare'],\n",
        "            'Healthcare Worker': ['Health Care', 'First Aid', 'Communication', 'Teamwork'],\n",
        "            'Marketing Specialist': ['Social Media', 'Communication', 'Writing', 'Arts & Crafts'],\n",
        "            'Chef': ['Cooking', 'Teamwork', 'Communication', 'Arts & Crafts'],\n",
        "            'Veterinarian': ['Animal Care', 'Health Care', 'Communication', 'First Aid'],\n",
        "            'Social Worker': ['Communication', 'Community Service', 'Volunteering', 'Public Speaking'],\n",
        "            'Personal Trainer': ['Fitness', 'Health Care', 'Communication', 'Teaching'],\n",
        "            'Construction Manager': ['Construction', 'Teamwork', 'Communication', 'Technology'],\n",
        "            'Writer': ['Writing', 'Communication', 'Reading', 'Arts & Crafts'],\n",
        "            'Childcare Provider': ['Childcare', 'Communication', 'First Aid', 'Teaching'],\n",
        "            'Event Coordinator': ['Communication', 'Fundraising', 'Public Speaking', 'Social Media'],\n",
        "            'Landscaper': ['Gardening', 'Construction', 'Teamwork', 'Fitness'],\n",
        "            'Travel Guide': ['Traveling', 'Communication', 'Public Speaking', 'Teaching'],\n",
        "            'Actor': ['Acting', 'Communication', 'Arts & Crafts', 'Public Speaking'],\n",
        "            'Singer': ['Singing', 'Arts & Crafts', 'Communication', 'Public Speaking'],\n",
        "            'Game Developer': ['Gaming', 'Programming', 'Technology', 'Arts & Crafts'],\n",
        "            'Sports Coach': ['Sports', 'Fitness', 'Teaching', 'Communication'],\n",
        "            'Volunteer Coordinator': ['Volunteering', 'Community Service', 'Communication', 'Fundraising']\n",
        "        }\n",
        "\n",
        "        logger.info(\"Preparing data...\")\n",
        "        X_text, X_skills, y_categories = self.prepare_data(df)\n",
        "\n",
        "        # Create match scores (based on skill overlap)\n",
        "        match_scores = []\n",
        "        for _, row in df.iterrows():\n",
        "            # Calculate match score based on skill relevance to job category\n",
        "            base_skills = job_skill_mapping.get(row['job_category'], [])\n",
        "            user_skills = row['required_skills']\n",
        "\n",
        "            # Calculate overlap\n",
        "            overlap = len(set(base_skills) & set(user_skills))\n",
        "            total_base = len(base_skills)\n",
        "\n",
        "            if total_base > 0:\n",
        "                match_score = overlap / total_base\n",
        "                # Add some noise\n",
        "                match_score += np.random.normal(0, 0.1)\n",
        "                match_score = np.clip(match_score, 0.0, 1.0)\n",
        "            else:\n",
        "                match_score = np.random.uniform(0.3, 0.7)\n",
        "\n",
        "            match_scores.append(match_score)\n",
        "\n",
        "        y_match = np.array(match_scores)\n",
        "\n",
        "        # Split data\n",
        "        indices = np.arange(len(X_text))\n",
        "        train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "\n",
        "        X_text_train, X_text_test = X_text[train_idx], X_text[test_idx]\n",
        "        X_skills_train, X_skills_test = X_skills[train_idx], X_skills[test_idx]\n",
        "        y_cat_train, y_cat_test = y_categories[train_idx], y_categories[test_idx]\n",
        "        y_match_train, y_match_test = y_match[train_idx], y_match[test_idx]\n",
        "\n",
        "        logger.info(\"Building model...\")\n",
        "        num_skills = X_skills.shape[1]\n",
        "        num_categories = len(self.job_category_encoder.classes_)\n",
        "\n",
        "        self.model = self.build_model(num_skills, num_categories)\n",
        "\n",
        "        logger.info(\"Model architecture:\")\n",
        "        self.model.summary()\n",
        "\n",
        "        # Callbacks\n",
        "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "\n",
        "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5,\n",
        "            min_lr=1e-6\n",
        "        )\n",
        "\n",
        "        logger.info(\"Training model...\")\n",
        "        try:\n",
        "            history = self.model.fit(\n",
        "                [X_text_train, X_skills_train],\n",
        "                [y_cat_train, y_match_train],\n",
        "                validation_data=([X_text_test, X_skills_test], [y_cat_test, y_match_test]),\n",
        "                epochs=30,  # Reduced epochs\n",
        "                batch_size=64,  # Larger batch size\n",
        "                callbacks=[early_stopping, reduce_lr],\n",
        "                verbose=1\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Training failed: {e}\")\n",
        "            # Try with smaller batch size\n",
        "            logger.info(\"Retrying with smaller batch size...\")\n",
        "            history = self.model.fit(\n",
        "                [X_text_train, X_skills_train],\n",
        "                [y_cat_train, y_match_train],\n",
        "                validation_data=([X_text_test, X_skills_test], [y_cat_test, y_match_test]),\n",
        "                epochs=20,\n",
        "                batch_size=32,\n",
        "                callbacks=[early_stopping, reduce_lr],\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "        logger.info(\"Evaluating model...\")\n",
        "        predictions = self.model.predict([X_text_test, X_skills_test])\n",
        "        cat_pred = np.argmax(predictions[0], axis=1)\n",
        "        match_pred = predictions[1].flatten()\n",
        "\n",
        "        cat_accuracy = accuracy_score(y_cat_test, cat_pred)\n",
        "        match_mae = np.mean(np.abs(y_match_test - match_pred))\n",
        "\n",
        "        logger.info(f\"Category prediction accuracy: {cat_accuracy:.4f}\")\n",
        "        logger.info(f\"Match score MAE: {match_mae:.4f}\")\n",
        "\n",
        "        # Print some example predictions\n",
        "        logger.info(\"Sample predictions:\")\n",
        "        for i in range(min(5, len(y_cat_test))):\n",
        "            true_cat = self.job_category_encoder.classes_[y_cat_test[i]]\n",
        "            pred_cat = self.job_category_encoder.classes_[cat_pred[i]]\n",
        "            logger.info(f\"  True: {true_cat} | Pred: {pred_cat} | Match Score: {match_pred[i]:.3f} (True: {y_match_test[i]:.3f})\")\n",
        "\n",
        "        # Save model and components\n",
        "        self.save_model(save_path)\n",
        "\n",
        "        return history\n",
        "\n",
        "    def predict(self, job_description: str, user_skills: List[str]) -> Dict:\n",
        "        \"\"\"Predict job category and match score\"\"\"\n",
        "        if not self.model:\n",
        "            raise ValueError(\"Model not trained or loaded\")\n",
        "\n",
        "        # Preprocess input\n",
        "        processed_desc = self.preprocess_text(job_description)\n",
        "        sequence = self.tokenizer.texts_to_sequences([processed_desc])\n",
        "        X_text = pad_sequences(sequence, maxlen=self.max_len)\n",
        "\n",
        "        # Encode skills\n",
        "        skill_vector = np.zeros((1, len(self.skill_categories)))\n",
        "        for skill in user_skills:\n",
        "            if skill in self.skill_categories:\n",
        "                idx = self.skill_categories.index(skill)\n",
        "                skill_vector[0, idx] = 1\n",
        "\n",
        "        # Make prediction\n",
        "        predictions = self.model.predict([X_text, skill_vector])\n",
        "\n",
        "        # Get results\n",
        "        category_probs = predictions[0][0]\n",
        "        match_score = predictions[1][0][0]\n",
        "\n",
        "        predicted_category = self.job_category_encoder.classes_[np.argmax(category_probs)]\n",
        "        confidence = np.max(category_probs)\n",
        "\n",
        "        return {\n",
        "            'predicted_category': predicted_category,\n",
        "            'confidence': float(confidence),\n",
        "            'match_score': float(match_score),\n",
        "            'category_probabilities': {\n",
        "                cat: float(prob)\n",
        "                for cat, prob in zip(self.job_category_encoder.classes_, category_probs)\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def save_model(self, save_path: str):\n",
        "        \"\"\"Save model and preprocessing components\"\"\"\n",
        "        import os\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "        # Save Keras model\n",
        "        self.model.save(f\"{save_path}/model.h5\")\n",
        "\n",
        "        # Save tokenizer\n",
        "        with open(f\"{save_path}/tokenizer.json\", \"w\") as f:\n",
        "            json.dump(self.tokenizer.to_json(), f)\n",
        "\n",
        "        # Save encoders\n",
        "        joblib.dump(self.skill_encoder, f\"{save_path}/skill_encoder.pkl\")\n",
        "        joblib.dump(self.job_category_encoder, f\"{save_path}/job_category_encoder.pkl\")\n",
        "\n",
        "        # Save configuration\n",
        "        config = {\n",
        "            'max_features': self.max_features,\n",
        "            'max_len': self.max_len,\n",
        "            'embedding_dim': self.embedding_dim,\n",
        "            'skill_categories': self.skill_categories\n",
        "        }\n",
        "\n",
        "        with open(f\"{save_path}/config.json\", \"w\") as f:\n",
        "            json.dump(config, f, indent=2)\n",
        "\n",
        "        logger.info(f\"Model saved to {save_path}\")\n",
        "\n",
        "    def load_model(self, save_path: str):\n",
        "        \"\"\"Load trained model and preprocessing components\"\"\"\n",
        "        # Load configuration\n",
        "        with open(f\"{save_path}/config.json\", \"r\") as f:\n",
        "            config = json.load(f)\n",
        "\n",
        "        self.max_features = config['max_features']\n",
        "        self.max_len = config['max_len']\n",
        "        self.embedding_dim = config['embedding_dim']\n",
        "        self.skill_categories = config['skill_categories']\n",
        "\n",
        "        # Load model\n",
        "        self.model = tf.keras.models.load_model(f\"{save_path}/model.h5\")\n",
        "\n",
        "        # Load tokenizer\n",
        "        with open(f\"{save_path}/tokenizer.json\", \"r\") as f:\n",
        "            tokenizer_json = json.load(f)\n",
        "\n",
        "        self.tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(tokenizer_json)\n",
        "\n",
        "        # Load encoders\n",
        "        self.skill_encoder = joblib.load(f\"{save_path}/skill_encoder.pkl\")\n",
        "        self.job_category_encoder = joblib.load(f\"{save_path}/job_category_encoder.pkl\")\n",
        "\n",
        "        logger.info(f\"Model loaded from {save_path}\")\n",
        "\n",
        "# Example usage and testing\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the system\n",
        "    job_matcher = JobMatchingSystem()\n",
        "\n",
        "    # First, test data generation\n",
        "    print(\"Testing data generation...\")\n",
        "    test_df = job_matcher.create_synthetic_data(num_samples=100)\n",
        "    print(f\"Generated {len(test_df)} samples\")\n",
        "    print(\"Sample data:\")\n",
        "    print(test_df.head())\n",
        "    print(f\"Job categories: {test_df['job_category'].unique()}\")\n",
        "\n",
        "    # Train the model\n",
        "    print(\"\\nTraining the job matching model...\")\n",
        "    try:\n",
        "        history = job_matcher.train(save_path='job_matching_model')\n",
        "        print(\"Training completed successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Training failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        exit(1)\n",
        "\n",
        "    # Test predictions\n",
        "    print(\"\\nTesting predictions:\")\n",
        "\n",
        "    test_cases = [\n",
        "        {\n",
        "            'description': \"We need a UI/UX designer with 2 years experience in mobile app design.\",\n",
        "            'skills': ['Arts & Crafts', 'Technology', 'Communication']\n",
        "        },\n",
        "        {\n",
        "            'description': \"Looking for a software developer to work on web applications.\",\n",
        "            'skills': ['Programming', 'Technology', 'Communication']\n",
        "        },\n",
        "        {\n",
        "            'description': \"Elementary teacher needed for our school.\",\n",
        "            'skills': ['Teaching', 'Childcare', 'Communication']\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    for i, test_case in enumerate(test_cases):\n",
        "        result = job_matcher.predict(test_case['description'], test_case['skills'])\n",
        "        print(f\"\\nTest Case {i+1}:\")\n",
        "        print(f\"Description: {test_case['description']}\")\n",
        "        print(f\"User Skills: {test_case['skills']}\")\n",
        "        print(f\"Predicted Category: {result['predicted_category']}\")\n",
        "        print(f\"Confidence: {result['confidence']:.3f}\")\n",
        "        print(f\"Match Score: {result['match_score']:.3f}\")"
      ]
    }
  ]
}